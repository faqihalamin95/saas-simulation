# SaaS Analytics Pipeline

A comprehensive analytics engineering project simulating real-world SaaS subscription data with intentional data quality challenges, built incrementally from local development to cloud production architecture.

---

## ğŸ¯ Overview

This project implements an end-to-end SaaS analytics pipeline built on synthetically generated subscription data.

Unlike typical portfolio projects that rely on pre-cleaned public datasets, this pipeline generates its own raw data, intentionally injects data quality issues, and processes them through a layered and reproducible architecture.

### The primary objective of this project is to demonstrate:

- deterministic pipeline design
- subscription-based dimensional modeling
- temporal & cohort analysis
- Slowly Changing Dimensions (SCD Type 2)
- data quality handling under schema instability
- incremental-to-scalable architectural thinking
- business-metric-driven transformation (MRR, churn, retention, LTV)

> **This is not a dashboard-first project.** 
> **This is an analytics engineering & pipeline design project.**

---

## ğŸ§± Architecture Overview

### The pipeline follows a layered architecture pattern:

```text
Synthetic Data Generator
â†“
Raw Layer
â†“
Staging Layer
â†“
Foundation Layer
â†“
Marts Layer
```

Each layer has explicit responsibility boundaries.

### The project is implemented in two architectural phases:

- **Phase 1 â€” Local Stack (Year 1)**

- **Phase 2 â€” Cloud Stack (Year 2)**

The second phase is intentionally dependent on business validation from the first phase.

---

## ğŸ—ï¸ Architectural Philosophy

### This project adopts a deliberate scaling strategy:

```text
Start local â†’ validate business â†’ scale infrastructure.
```

Infrastructure investment is treated as **a consequence of validated unit economics**, not **a starting assumption**.

### This mirrors real startup evolution:

- validate with minimal infrastructure
- only scale once justified by data

---

## ğŸ² Data Generation Strategy

Instead of consuming external CSV data, this project builds **a synthetic SaaS simulator**.

### Generator Responsibilities

- User lifecycle simulation:

    Trial â†’ Conversion â†’ Upgrade/Downgrade â†’ Churn â†’ Reactivation

- Payment simulation:

    - Success/failure logic
    - Retry rules
    - 3-strike cancellation policy

- Product usage event tracking
- Multi-timezone handling (7 countries)

### Weighted probabilities simulate realistic SaaS behavior:

- Trial conversion â‰ˆ 40%
- Monthly churn â‰ˆ 12%
- Seasonal Q4 slowdown
- Multiple acquisition channels

The generator produces realistic but controlled complexity.

---

## âš ï¸ Chaos Engineering (Data Quality Simulation)

**This project intentionally injects data quality issues to simulate real-world instability.**

### Scenario:
- Late-arriving events
- Schema evolution, because new columns added mid-year	
- Duplicate payments	
- Type drift: Numeric â†’ String	
- Plan rename, from business rebrand	

Not all issues are "fixed".

Some are:
- documented
- tracked
- made observable

The goal is pipeline resilience.

---

## ğŸ“Š Project Structure

```text
.
â”œâ”€â”€ README.md                          
â”œâ”€â”€ 01-local-stack/                    # Year 1 implementation
â”‚   â”œâ”€â”€ README.md                      # Local stack details
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ generator/                 # Synthetic data simulator
â”‚   â”‚   â”œâ”€â”€ ingestion/
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”œâ”€â”€ dbt/                           # dbt project
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ staging/               # Clean & cast raw data
â”‚   â”‚   â”‚   â”œâ”€â”€ foundation/            # Star schema (dims + facts)
â”‚   â”‚   â”‚   â””â”€â”€ marts/                 # Business metrics
â”‚   â”‚   â”œâ”€â”€ seeds/                     # Static reference data
â”‚   â”‚   â””â”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ raw/                       # Generated parquet files
â”‚   â”‚   â””â”€â”€ reports/                   # Validation outputs
â”‚   â””â”€â”€ Makefile                       # Task orchestration
â”‚   
â”‚
â”œâ”€â”€ 02-cloud-stack/                    # Year 2 
â””â”€â”€ requirements.txt
```

---

## ğŸ› ï¸ Tech Stack 

### Phase 1 â€“ Local

- Python (data generation)
- Pandas
- PostgreSQL 16
- dbt Core
- Makefile (local orchestration)
- DBeaver (inspection)

The stack is intentionally simple and reproducible.

### Phase 2 â€“ Cloud Evolution

Triggered only if:

Year 1 demonstrates sustainable unit economics.

Planned enhancements:

- Apache Airflow orchestration
- Snowflake
- dbt Cloud
- Real-time dashboards
- CI/CD integration

**10Ã— data volume simulation**
Business validation precedes infrastructure scaling.

---

ğŸ“ˆ Core Business Question

**The entire pipeline exists to answer:**

- Is this SaaS business sustainable for scaling?

All marts and transformations are derived from this question.

This prevents scope creep and metric inflation.

---

## ğŸ“ What This Project Demonstrates

- **Subscription-based dimensional modeling**
- **SCD Type 2 implementation**
- **Temporal revenue attribution**
- **Chaos-aware pipeline design**
- **Business-driven analytics engineering**
- **Incremental architecture thinking (local â†’ cloud)**

---

## ğŸ”® Design Philosophy

### This project prioritizes:

- Explicit modeling over implicit assumptions
- Deterministic builds over uncontrolled incrementals
- Business-first metrics over vanity dashboards
- Simulated realism over sanitized datasets
- Architectural evolution over premature cloud complexity

Complexity is introduced intentionally â€” not accidentally.

---

## ğŸ“Œ Notes

This project is designed as an advanced analytics engineering portfolio project demonstrating subscription analytics modeling under real-world instability conditions.

**It is intentionally built in stages to reflect how real systems evolve.**